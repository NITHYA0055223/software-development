# -*- coding: utf-8 -*-
"""software development.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rLHVouhyb_V6WJgW0tX5kBKQiwFizfuk
"""

!pip install transformers torch gradio -q

import io
import sys
import traceback
from typing import Optional

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Global model/tokenizer handles (will be set by load_model)
MODEL = None
TOKENIZER = None
MODEL_DEVICE = "cpu"


def load_model(model_name: str = "gpt2",
               dtype: Optional[str] = None,
               use_cuda_if_available: bool = True):
    """
    Load tokenizer and causal LM into global variables.

    model_name: Hugging Face repo id (eg "gpt2" or "google/flan-t5-small" but this code
                expects a causal LM compatible with AutoModelForCausalLM).
    dtype: "float16"|"float32" or None -> auto
    """
    global MODEL, TOKENIZER, MODEL_DEVICE

    # Decide device
    has_cuda = torch.cuda.is_available() and use_cuda_if_available
    MODEL_DEVICE = "cuda" if has_cuda else "cpu"

    # dtype selection
    dtype_map = {
        "float16": torch.float16,
        "fp16": torch.float16,
        "float32": torch.float32,
        "fp32": torch.float32,
        "bfloat16": torch.bfloat16,
    }
    torch_dtype = dtype_map.get(dtype, None)

    try:
        TOKENIZER = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        MODEL = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch_dtype,
            device_map="auto" if has_cuda else None,
            low_cpu_mem_usage=not has_cuda
        )
        # If tokenizer has no pad token set pad to eos token to avoid warnings
        if TOKENIZER.pad_token_id is None:
            TOKENIZER.pad_token = TOKENIZER.eos_token

        return f"Loaded model '{model_name}' on {MODEL_DEVICE}."
    except Exception as e:
        tb = traceback.format_exc()
        return f"Error loading model '{model_name}': {e}\n\nTraceback:\n{tb}"


def generate_response(prompt: str,
                      max_new_tokens: int = 256,
                      temperature: float = 0.7,
                      do_sample: bool = True):
    """
    Generate text from the loaded model. Returns a string.
    """
    global MODEL, TOKENIZER, MODEL_DEVICE
    if MODEL is None or TOKENIZER is None:
        return "Model not loaded. Please load a model first."

    try:
        inputs = TOKENIZER(prompt, return_tensors="pt", truncation=True)
        # Move tensors to model device
        device = next(MODEL.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            generated = MODEL.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=do_sample,
                pad_token_id=TOKENIZER.eos_token_id,
                # return_dict_in_generate=True, output_scores=True  # optional
            )
        text = TOKENIZER.decode(generated[0], skip_special_tokens=True)
        # Remove the prompt portion if model echoes it
        if text.startswith(prompt):
            text = text[len(prompt):].strip()
        return text.strip()
    except Exception as e:
        tb = traceback.format_exc()
        return f"Error during generation: {e}\n\nTraceback:\n{tb}"


def extract_text_from_pdf(pdf_file) -> str:
    """
    Extract text from an uploaded PDF file object (file-like).
    Returns extracted text or an error message.
    """
    if pdf_file is None:
        return ""

    try:
        # pdf_file may be a tempfile-like object or path string depending on Gradio.
        if isinstance(pdf_file, str):
            f = open(pdf_file, "rb")
            close_after = True
        else:
            f = pdf_file
            close_after = False

        reader = PyPDF2.PdfReader(f)
        text_parts = []
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text_parts.append(page_text)
        if close_after:
            f.close()
        return "\n\n".join(text_parts).strip()
    except Exception as e:
        tb = traceback.format_exc()
        return f"Error reading PDF: {e}\n\nTraceback:\n{tb}"


def eco_tips_generator(problem_keywords: str,
                       max_tokens: int = 400,
                       temperature: float = 0.7) -> str:
    """
    Generate practical eco-friendly tips given keywords or short description.
    """
    if not problem_keywords or problem_keywords.strip() == "":
        return "Please enter some environmental problem keywords or short description."

    prompt = (
        "You are an expert sustainability advisor. "
        "Generate practical, actionable, and specific eco-friendly tips and solutions "
        f"related to this environmental problem or keywords: {problem_keywords}\n\n"
        "Structure the answer with short bullet points. For each bullet point include:\n"
        "- A short actionable tip (1 sentence)\n"
        "- A concise explanation (1-2 sentences)\n"
        "- Example or next steps where appropriate\n\n"
        "Be concrete, low-cost where possible, and prioritize measurable impact."
        "\n\nAnswer:\n"
    )
    return generate_response(prompt, max_new_tokens=max_tokens, temperature=temperature)


def policy_summarization(pdf_file,
                         policy_text: str,
                         max_tokens: int = 600,
                         temperature: float = 0.2) -> str:
    """
    Summarize a policy document (from PDF and/or pasted text). If both provided,
    the PDF content and policy_text are concatenated for summarization.
    """
    extracted = extract_text_from_pdf(pdf_file) if pdf_file else ""
    combined = ""
    if extracted:
        combined += f"PDF CONTENT:\n{extracted}\n\n"
    if policy_text and policy_text.strip():
        combined += f"ADDITIONAL POLICY TEXT:\n{policy_text.strip()}\n\n"
    if not combined:
        return "Please upload a PDF or paste policy text to summarize."

    prompt = (
        "You are an expert policy analyst. Summarize the following policy document and extract the most "
        "important points, key provisions, implementation implications, and any potential challenges.\n\n"
        f"{combined}\n\n"
        "Deliverable:\n"
        "1) A short executive summary (3-6 sentences)\n"
        "2) A bullet list of the top 8 key provisions / points (each 1-2 sentences)\n"
        "3) A brief list of implementation implications and recommendations (3-5 bullets)\n"
    )
    return generate_response(prompt, max_new_tokens=max_tokens, temperature=temperature)


# --- Gradio UI setup ---
def build_ui():
    with gr.Blocks() as demo:
        gr.Markdown("# AI Assistant & Policy Analyzer")
        gr.Markdown(
            "Load a compatible Hugging Face causal LM (AutoModelForCausalLM). "
            "Use the tabs below to generate eco tips or summarize policy PDFs."
        )

        with gr.Row():
            model_name_input = gr.Textbox(label="Model name (Hugging Face repo id)", value="gpt2")
            dtype_input = gr.Dropdown(choices=["auto", "float16", "float32"], value="auto",
                                      label="Preferred dtype (auto chooses)")
            load_btn = gr.Button("Load model")
            model_status = gr.Textbox(label="Model load status", interactive=False)

        def _load_model_click(model_name, dtype_choice):
            dtype = None if dtype_choice == "auto" else dtype_choice
            return load_model(model_name.strip() or "gpt2", dtype)

        load_btn.click(_load_model_click, inputs=[model_name_input, dtype_input], outputs=[model_status])

        with gr.Tabs():
            with gr.TabItem("Eco Tips Generator"):
                gr.Markdown("Generate practical eco-friendly tips from problem keywords.")
                with gr.Row():
                    keywords = gr.Textbox(label="Environmental problem / keywords", placeholder="e.g. single-use plastics, food waste")
                    max_tokens_tips = gr.Slider(minimum=50, maximum=1500, value=400, step=10, label="Max tokens for response")
                    temp_tips = gr.Slider(minimum=0.0, maximum=1.2, value=0.7, step=0.05, label="Temperature")
                generate_btn = gr.Button("Generate Eco Tips")
                tips_output = gr.Textbox(label="Eco Tips (output)", lines=12)
                generate_btn.click(eco_tips_generator, inputs=[keywords, max_tokens_tips, temp_tips], outputs=[tips_output])

            with gr.TabItem("Policy Summarizer & Extractor"):
                gr.Markdown("Upload a policy PDF and/or paste policy text to get a concise summary and key points.")
                with gr.Row():
                    pdf_upload = gr.File(label="Upload policy PDF (optional)", file_types=[".pdf"])
                    policy_paste = gr.Textbox(label="Paste policy text (optional)", lines=8, placeholder="Paste text here...")
                with gr.Row():
                    max_tokens_policy = gr.Slider(minimum=50, maximum=2000, value=600, step=10, label="Max tokens for summary")
                    temp_policy = gr.Slider(minimum=0.0, maximum=1.0, value=0.2, step=0.05, label="Temperature (low = deterministic)")
                summarize_btn = gr.Button("Summarize Policy")
                policy_output = gr.Textbox(label="Policy Summary & Key Points", lines=18)
                summarize_btn.click(policy_summarization, inputs=[pdf_upload, policy_paste, max_tokens_policy, temp_policy], outputs=[policy_output])

        gr.Markdown("— End of app —")
    return demo


if __name__ == "__main__":
    demo = build_ui()
    # Launch Gradio
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)